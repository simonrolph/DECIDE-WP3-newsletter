---
title: Preparation for MyDECIDE generation
output: html_document
---

## Introduction

This script creates the data outputs that are then passed to `2_render_newsletter_slurm.R` or `2_render_newsletter_job.R` when actually does the rendering. Whilst this is set up as R markdown, it is not intended to just be run as one but running each chunk separately (in a notebook sort of manner).

Step 0: some preparation work that needs to be done outside this script

Step 1: Load the user data and the records data into the R environment. Apply a 'top-up' step to get any records missed by the daily data collector.

Step 2: Determine what email each user should get 

Step 3: Save outputs that are then passed to the next processing stage

Step 4: Tests

## Set up

Initial set up, packages.

We define the 30 day period that the newsletters are generated from. This can be changed to be today or not today.

```{r}
library(blastula)
library(rmarkdown)
library(readr)
library(fst)
library(dplyr)
library(sf)
library(httr)
library(jsonlite)
library(curl)

library(terra) # for get_decide_score_local

#for parallel rendering
library(doParallel)

#the 30 day period
start_date <- Sys.Date()-31
end_date <- Sys.Date()-1

start_date <- as.Date("2022-07-11")
end_date <- as.Date("2022-08-10")

```

## Step 0:

Go to the MyDECIDE sign-up app and download the latest set of users. Save this file in `data_personal`. Update the line in the next code chunk to reflect this.

Open `data-personal/users_lookups.csv` in Excel, add new users from the file downloaded from the sign-up portal and assign user tracks (t1-18).

## Step 1: Generate the records data frame

### Load users

Load in the users from a file downloaded from the sign-up app.

```{r}
#load dataframe
user_db <- read_csv("data_personal/data-2022-08-11.csv") #<-- update this to be the latest set of users
names(user_db)[1] <- "user_id"

#remove spaces from irecord username
user_db$irecord_username <- gsub(" ","",user_db$irecord_username)

#remove doublespaces from names
user_db$name <- gsub("  "," ",user_db$name)

#remove duplicate users (https://stackoverflow.com/questions/13279582/select-the-first-row-by-group)
user_db <-user_db[!duplicated(user_db$email),]


#create a new column which will be used to store the file location of outputted email
user_db$newsletter_file_location <- ""

#only subscribed users
user_db <- filter(user_db,subscribed)

#View the data frame
View(user_db)
```

### Build records data frame

Build the records data from the data on the SAN created by the daily data collector. This produces a big data frame with records, their locations, who recorded them and their DECIDE score

```{r}
#local verson of updated records (non-seasonal only)
#this includes the non-seasonal backtracker records and the top-up records
file_location <- "../DECIDE-dynamic-dataflow/data/data_cache/butterfly"
files_from_local <- list.files(file_location) %>%
  paste(file_location,.,sep="\\") 

#getting data from live server
#note all the extra back slashes for escaping
file_location_san <- "\\\\nerclactdb.adceh.ceh.ac.uk\\appdev\\appdev\\DECIDE\\data\\species_data\\data_cache\\butterfly"
files_from_san <- list.files(file_location_san)[!(list.files(file_location_san) %in% list.files(file_location))] %>%
  paste(file_location_san,.,sep="\\") 

#load all the data and put it together into one dataframe
records <- c(files_from_local,files_from_san) %>%
  lapply(read_fst) %>%
  lapply(function(x){x$user <- as.character(x$user); x}) %>%
  bind_rows() %>% filter(!is.na(user))

glimpse(records)

#Format columns

#get easting northings
records <- st_as_sf(records,coords = c('longitude', 'latitude'), crs = 4326)  #sf version
records[,c("longitude","latitude")] <- st_coordinates(records)

records  <- records %>% st_transform(27700)
records[,c("easting","northing")] <- st_coordinates(records)
records <- st_drop_geometry(records)

#get day/month/year columns
records$day <- records$observed_on %>% as.Date() %>% format(format = "%d")
records$month <- records$observed_on %>% as.Date() %>% format(format = "%m")
records$year <- records$observed_on %>% as.Date() %>% format(format = "%Y")

#save data
saveRDS(records,paste0("data/species_records_",end_date,".RDS"))
saveRDS(records,paste0("data/species_records.RDS"))

```

### 'Top-up' records to fill gaps

Due to an unknown error, the data collector doesn't seem to get all the records, although it is much better than when it was first operating. Therefore we need to manually go through each user to get their records. This is then compiled for each recording platform, combined, DECIDE score retrieved and saved to a .fst with a random number appended. This is wrapped in a `if(F){}` because it is an optional step but makes sure no records have been missed and therefore opportunity to send a personalised email isn't missed.

This is assuming that you are running this chunk directly after the previous chunk so that the `records` object is already loaded into the environment.

```{r}
if(F){
  average_decide <- records$decide_score_all_time_pre %>% mean(na.rm = T)

  source("functions/get_data.R")
  source("functions/get_records.R")
  
  start_date_top_up <- as.Date("2022-04-01")
  
  #GET iRecord top up
  #users which need irecord records
  irecord_usernames <- user_db$irecord_username %>% unique()
  irecord_usernames <- irecord_usernames[!is.na(irecord_usernames)]
  irecord_data <- data.frame()
  
  #loop through usernames and make API calls
  for (i in 1:length(irecord_usernames)){
    new_irecord_data <- get_records_irecord(irecord_usernames[i],1000,gsub("Â","",Sys.getenv("irecord_key")),start_date_top_up,end_date,raw=F)
    if(nrow(new_irecord_data)>0){
      new_irecord_data$user <- irecord_usernames[i]
      irecord_data <- bind_rows(irecord_data,new_irecord_data)
    }
    
    print(paste0(i,"/",length(irecord_usernames)))
    Sys.sleep(1)
  }
  
  #GET ispot top-up
  #users which need iSpot records
  ispot_usernames <- user_db$ispot_username %>% unique()
  ispot_usernames <- ispot_usernames[!is.na(ispot_usernames)]
  ispot_data <- data.frame()
  
  #loop through usernames and make API calls
  for (i in 1:length(ispot_usernames)){
    new_ispot_data <- get_records_ispot(ispot_usernames[i],1000,Sys.getenv("ispot_key"),start_date_top_up,end_date)
    
    if(nrow(new_ispot_data)>0){
      new_ispot_data$user <- ispot_usernames[i]
      ispot_data <- bind_rows(ispot_data,new_ispot_data)
    }
    
    print(paste0(i,"/",length(ispot_usernames)))
    Sys.sleep(5)
  }
  
  #GET inat top-up
  #users which need inat records
  inat_usernames <- user_db$inat_username %>% unique()
  inat_usernames <- inat_usernames[!is.na(inat_usernames)]
  inat_data <- data.frame()
  
  #loop through usernames and make API calls
  for (i in 1:length(inat_usernames)){
    new_inat_data <- get_records_inat(inat_usernames[i],1000)
    
    if(nrow(new_inat_data)>0){
      new_inat_data$user <- inat_usernames[i]
      inat_data <- bind_rows(inat_data,new_inat_data)
    }
    
    print(paste0(i,"/",length(inat_usernames)))
    Sys.sleep(5)
  }
  
  
  #filter to relevant species
  species_list <- readRDS("data/species_list.RDS") %>% filter(group == "butterfly") %>% pull(species)
  
  #combine data from different sources
  new_data <- bind_rows(irecord_data,ispot_data,inat_data) %>%
    unique() %>%
    filter(scientific_name %in% species_list) %>%
    select(scientific_name,latitude,longitude,observed_on,user,longitude,latitude) %>%
    mutate(user = as.character(user))
  
  #which records are missing from the main data sets?
  missing_records <- setdiff(new_data,records %>% select(scientific_name,latitude,longitude,observed_on,user,longitude,latitude))
  
  nrow(missing_records)
  
  #get the decide score for the missing records
  source("functions/get_decide_score.R")
  
  
  #get decide score for the missing records
  for (i in 1:nrow(missing_records)){
    score <- get_decide_score_local(missing_records$longitude[i],missing_records$latitude[i],"//nerclactdb.adceh.ceh.ac.uk/appdev/appdev/DECIDE/data/species_data/raster_decide_priority/butterfly_decide_raster_all_year.tif")
    
    if(!is.null(score)){
      missing_records$decide_score[i] <- score
    } else {
      missing_records$decide_score[i] <- average_decide
    }
    
    print(i)
  }
  
  #create matching columns for the main data frame
  missing_records <- missing_records %>% mutate(record_id = NA,
                                                created_on = NA, 
                                                platform = NA, 
                                                days_old = NA,
                                                days_since_2000 = NA,
                                                group = "butterfly",
                                                observed_month_num = NA,
                                                decide_score_seasonal_pre = decide_score,
                                                decide_score_all_time_pre = NA,
                                                decide_score_seasonal_post = decide_score,
                                                decide_score_all_time_post = NA) %>%
    select(names(records)[1:16])
    
   
  #save missing records
  write_fst(missing_records,paste0("../DECIDE-dynamic-dataflow/data/data_cache/butterfly/top_up_records",round(runif(1)*10000000),".fst"))
}

#Then go back and run the previous chunk to compile the big dataset



```

## Step 2: Determine what email to send to users

### Load data

Using the mass records dataframe (`records`), the latin square (`latin_square`) formulation and a dataframe of what track each user is on (`user_tracks`) we determine which email they should receive next given what they have already received `email_log`

```{r}
#look-up table for data story type versus letter
all_data_stories <- expand.grid(personalised = c(T,F),ds =1:5)[c(1:8,10),]
all_data_stories$letter <- LETTERS[1:9]

#load in the latin square and refomrat to long format
latin_square <- read.csv("latin_square.csv",header = F)
names(latin_square) <- paste0(1:9) 
latin_square$track <- paste0("t",1:18)
latin_square <- latin_square %>% tidyr::pivot_longer(cols =1:9)
names(latin_square)[2:3] <- c("seq","ds_letter")
latin_square$seq <- as.numeric(latin_square$seq)

#load the user database which contains their email and what track (t1-t18) they're on
user_tracks <- read.csv("data_personal/users_lookups.csv") %>% mutate(email = gsub(" ","",email))


#build email log from the rendered files
sent_emails <- data.frame(uid=0,personalised=F,ds="1")[-1,]
past_news_dates <- list.files("newsletters")
for(date_folder in past_news_dates){
  sent_emails_i <- list.files(paste0("newsletters/",date_folder)) %>% 
    read.table(text = .,sep = "_",as.is=T) 
  names(sent_emails_i) <- c("uid","personalised", "ds")
  
  sent_emails_i <- sent_emails_i %>% mutate(ds = gsub(".html","",ds))
  sent_emails_i$date <- as.Date(date_folder)
  sent_emails <- bind_rows(sent_emails,sent_emails_i)
}

sent_emails <- mutate(sent_emails, 
                      uid = as.numeric(uid),
                      ds = as.numeric(ds)) %>% 
  arrange(uid) %>% 
  left_join(all_data_stories)

email_log <- sent_emails %>% rename(data_story = letter)


```

### 'Potential to personalise' checks

Check whether the users have recorded in the last 30 days, or since 1st april 2022 (for data story 4).

```{r}

#load in data
records_data <- readRDS(paste0("data/species_records_",end_date,".RDS"))

#get a list of users that recorded this year (for data story 4)
thisyear <- records_data %>%
    filter(observed_on > as.Date("2022-04-01")) %>%
  pull(user) %>%
  unique()

#get a list of users that recorded in the past 30 days (for data stories 1-3)
last30days <- records_data %>%
    filter(observed_on < end_date,
           observed_on > start_date) %>%
  pull(user) %>%
  unique()

#create a new column that denotes whether each user in MyDECIDE has recorded in the past 30 days
user_db$recorded_recently <- user_db$irecord_username %in% last30days | 
  user_db$ispot_username %in% last30days |
  user_db$inat_username %in% last30days

#create a new column that denotes whether each user in MyDECIDE has recorded since 1/4/2022
user_db$recorded_this_year <- user_db$irecord_username %in% thisyear | 
  user_db$ispot_username %in% thisyear |
  user_db$inat_username %in% thisyear

View(user_db)

```

### Build the `emails` dataframe which outlines who is going to get what

This dataframe is built from the `user_db` combined with information in the `email_log` and the latin square / user tracks.

```{r}
#create the dataframe of emails that need to be sent
emails <- user_db

# identify what track each user is on
emails <- left_join(emails,user_tracks,by="email")

#new column for denoting which treatment is next
emails$letter <-""

# identify what the last email they received was
last_emails <- email_log %>% 
  group_by(uid) %>%
  filter(date == max(date)) %>% na.omit()

#identify the next email for them
for (i in 1:nrow(emails)){
  #what user are we working out
  user_uid <- emails$uid[i]
  
  #their track
  track <- latin_square %>% filter(track ==emails$group[i])
  
  #if they have been sent an email
  if(user_uid %in% last_emails$uid){
    #last email sent to them
    last_email <- last_emails %>% filter(uid == user_uid) %>% pull(data_story)
    
    #position on track
    track_position <- track %>% filter(ds_letter == last_email) %>% pull(seq)
  } else { # if they haven't been sent an email  
    track_position <- 0
  }
  
  #loop around
  track$seq[track$seq<=track_position] <- track$seq[track$seq<=track_position]+9
    
  next_emails <- track %>% arrange(seq) %>% pull(ds_letter)
  
  
  #which data stories do they have the data for?
  if(!emails$recorded_recently[i]){
    next_emails <- next_emails[!(next_emails %in% c("A","C","E"))];
  }
  
  if(!emails$recorded_this_year[i]){
    next_emails <- next_emails[next_emails != "G"]
  }
  
  next_email <- first(next_emails)
  
  emails$letter[i] <- next_email 
  
  
}

emails <- left_join(emails,all_data_stories)

#remove unsubscribed users
emails <- emails %>% filter(subscribed)

View(emails)

```

## Step 3: Produce outputs for next processing stage

### Export information for the newsletter generator script

Here we're looping through each user and making a nested list object that contains the name of the file for when the html is rendered and the markdown parameters in a list.
 
```{r}
emails <- emails %>% arrange(user_id)
n_emails <- nrow(emails)


markdown_params_list <- list()

#make a list of lists of markdown params
for (i in 1:n_emails){
  #get all the markdown parameters into a list
  markdown_params <- 
    list(
        name = emails$name[i],
        irecord_username = emails$irecord_username[i],
        ispot_username = emails$ispot_username[i],
        inat_username = emails$inat_username[i],
        start_date = start_date,
        end_date =  end_date,
        try_personalised = emails$personalised[i],
        records_data_location = paste0("data/species_records_",end_date,".RDS"),
        home_lat = emails$home_lat[i],
        home_lon = emails$home_lon[i],
        irecord_key      = gsub("Â","",Sys.getenv("irecord_key")),
        ispot_key        = Sys.getenv("ispot_key"),
        data_story = as.numeric(emails$ds[i]),
        user_uuid = emails$uid[i],
        letter = emails$letter[i]
    )
  
  out_file_name <- paste0("../newsletters/",
                        Sys.Date(),
                        "/",
                        emails$uid[i],
                        "_",
                        markdown_params$try_personalised,
                        "_",
                        markdown_params$data_story,
                        ".html")
  
  #if any usernames are not there then set them to NA
  if (length(markdown_params$irecord_username)==0){markdown_params$irecord_username <- NA}
  if (length(markdown_params$ispot_username)==0){markdown_params$ispot_username <- NA}
  if (length(markdown_params$inat_username)==0){markdown_params$inat_username <- NA}
  
  markdown_params_list[[i]] <- list(out = out_file_name,params = markdown_params)
}

#save the emails data frame which is used when dispatching emails
emails <- emails %>% mutate(newsletter_file_location = paste0("C:/Users/simrol/Documents/R/DECIDE-WP3-newsletter/newsletters/",end_date,"/",uid,"_",personalised,"_",ds,".html"))

#save this for reference
saveRDS(emails,paste0("data_personal/emails_",end_date,".rds"))

#save the params list - this is needed for the next stage!
saveRDS(markdown_params_list,"data_personal/markdown_params_list.rds")

#how many jobs to run on JASMIN
length(markdown_params_list)

```

## Next processing stage

The items needed for the next stage are:

 * The `records_{DATE}.RDS` object
 * The parameters for the `markdown_params_list` saved as `data_personal/markdown_params_list.rds`.
 
The items needed for sending the emails
 * The `emails` object saved as `data_personal/emails_{DATE}.rds`
 
The next stage is delivered with `2_render_newsletter_slurm.R` but on JASMIN (not running locally)
